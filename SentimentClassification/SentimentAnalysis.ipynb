{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in the Browser\n",
    "\n",
    "In this notebook, we will show how to create a `.air` file to perform sentiment analysis in the browser using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "For this notebook, the following dependencies are required:\n",
    "\n",
    "- `scikit-learn`\n",
    "- `tensorflow`\n",
    "- `aisquared`\n",
    "\n",
    "All of these are available on [pypi](https://pypi.org) via `pip`.  The following cell also runs the commands to install these dependencies as well as imports them into the notebook environment, along with TensorFlow (which is a dependency of the `mann` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwrenn4/miniforge3/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import aisquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Now that the required packages have been installed and imported, it is time to create the sentiment analysis model.  To do this, we have to first download and preprocess the data, train the model, and then package the model in the `.air` format.  The following cells will go through an in-depth explanation of each of the steps in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words = 10000,\n",
    "    skip_top = 0,\n",
    "    start_char = 1,\n",
    "    oov_char = 2,\n",
    "    index_from = 3\n",
    ")\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train,\n",
    "    maxlen = 512,\n",
    "    padding = 'post',\n",
    "    truncating = 'post'\n",
    ")\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test,\n",
    "    maxlen = 512,\n",
    "    padding = 'post',\n",
    "    truncating = 'post'\n",
    ")\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Add 2 to each vocab value to ensure matching with the needed values\n",
    "vocab = {\n",
    "    k : v + 2 for k, v in vocab.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 512, 4)            40000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 827,969\n",
      "Trainable params: 827,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "input_layer = tf.keras.layers.Input(x_train.shape[1:])\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    10000,\n",
    "    4\n",
    ")(input_layer)\n",
    "x = tf.keras.layers.Flatten()(embedding_layer)\n",
    "for _ in range(5):\n",
    "    x = tf.keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "output_layer = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(input_layer, output_layer)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 08:18:55.282574: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 29s 706ms/step - loss: 0.6921 - accuracy: 0.5110 - val_loss: 0.6856 - val_accuracy: 0.5420\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 28s 691ms/step - loss: 0.5290 - accuracy: 0.7290 - val_loss: 0.4790 - val_accuracy: 0.8010\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 25s 623ms/step - loss: 0.2701 - accuracy: 0.8917 - val_loss: 0.3707 - val_accuracy: 0.8472\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 27s 666ms/step - loss: 0.1445 - accuracy: 0.9506 - val_loss: 0.4777 - val_accuracy: 0.8452\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 25s 620ms/step - loss: 0.0700 - accuracy: 0.9778 - val_loss: 0.7303 - val_accuracy: 0.8324\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 23s 591ms/step - loss: 0.0474 - accuracy: 0.9849 - val_loss: 0.6667 - val_accuracy: 0.8454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1489c0ca0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train.reshape(-1,1),\n",
    "    epochs = 20,\n",
    "    batch_size = 512,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(min_delta = 0.001, patience = 3, restore_best_weights = True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Test Data:\n",
      "\n",
      "\n",
      "[[10750  1750]\n",
      " [ 2239 10261]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84     12500\n",
      "           1       0.85      0.82      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model performance\n",
    "preds = (model.predict(x_test) >= 0.5).astype(int)\n",
    "print('Model Performance on Test Data:')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Save the model\n",
    "model.save('SentimentClassifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package the Model\n",
    "\n",
    "Now that the model has been created, we can package the model into a single `.air` file that enables integration into the browser.\n",
    "\n",
    "To perform this packaging, we will be utilizing the `aisquared` package `DocumentPredictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvester = aisquared.config.harvesting.TextHarvester()\n",
    "\n",
    "preprocessor = aisquared.config.preprocessing.text.TextPreprocessor(\n",
    "    [\n",
    "        aisquared.config.preprocessing.text.Tokenize(),\n",
    "        aisquared.config.preprocessing.text.ConvertToCase(lowercase = True),\n",
    "        aisquared.config.preprocessing.text.RemoveCharacters(),\n",
    "        aisquared.config.preprocessing.text.ConvertToVocabulary(vocabulary = vocab, max_vocab = 9999),\n",
    "        aisquared.config.preprocessing.text.PadSequences(length = 512, pad_location = 'post', truncate_location = 'post')\n",
    "    ]\n",
    ")   \n",
    "analytic = aisquared.config.analytic.LocalModel('SentimentClassifier.h5', 'text')\n",
    "\n",
    "##we might be missing an analogue to \"sequence_length\" in the .config package\n",
    "\n",
    "postprocessor = aisquared.config.postprocessing.BinaryClassification(['positive', 'negative'], 0.5)\n",
    "\n",
    "renderer = aisquared.config.rendering.DocumentRendering(include_probability = True)\n",
    "\n",
    "feedback = aisquared.config.feedback.BinaryFeedback(['positive', 'negative'])\n",
    "\n",
    "aisquared.config.ModelConfiguration(\n",
    "    'SentimentClassifier',\n",
    "    harvester,\n",
    "    preprocessor,\n",
    "    analytic,\n",
    "    postprocessor,\n",
    "    renderer,\n",
    "    feedback).compile(dtype = 'float16')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0671325c08d22fc44ce2e58aedbf8efae69ce5eb9c1911bbe321ecb24080d883"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
